{
  "openapi": "3.0.3",
  "info": {
    "title": "Get watsonx ai response and relevant source",
    "description": "Get generative AI response and relevant documents",
    "version": "0.0.1"
  },
  "servers": [
    {
      "url": "https://application-8b.1k4k4t5r87u4.us-south.codeengine.appdomain.cloud",
      "description": "IP address and port without encryption/authentication",
      "variables": {
        "local_url": {
          "default": "0.0.0.0:8001",
          "description": "The portions of URL that follow http://"
        }
      }
    }
  ],
  "paths": {
    "/generation_stream": {
      "post": {
        "summary": "Post query to retrieve data",
        "description": "Get generative AI response and relevant documents",
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "input": {
                    "type": "string",
                    "description": "Get generative AI response and relevant documents"
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Default Response",
            "content": {
              "text/event-stream": {
                "schema": {
                  "$ref": "#/components/schemas/TextGenResponse"
                }
              }
            }
          },
          "default": {
            "description": "Unexpected error"
          }
        }
      }
    }
  },
  "components": {
    "schemas": {
      "TextGenRequest": {
        "type": "object",
        "required": [
          "model_id",
          "input",
          "project_id"
        ],
        "properties": {
          "model_id": {
            "type": "string",
            "description": "The ID of the model to be used for this request. Please refer to the list of models at https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-prompt-lab.html?context=wx",
            "example": "google/flan-ul2"
          },
          "input": {
            "type": "string",
            "description": "The input is the prompt to generate completions. Note: The method tokenizes the input internally. It is recommended not to leave any trailing spaces."
          },
          "project_id": {
            "type": "string",
            "description": "id for the associated watsonx project.",
            "minLength": 1,
            "maxLength": 255,
            "pattern": "^[a-zA-Z0-9_-]*$",
            "example": "3e992422-d337-47f9-869a-0928e49a3ea6"
          },
          "parameters": {
            "type": "object",
            "properties": {
              "decoding_method": {
                "type": "string",
                "description": "The strategy used for picking the tokens during generation of the output text.",
                "example": "greedy"
              },
              "random_seed": {
                "type": "integer",
                "description": "The random number generator seed to use in sampling mode for experimental repeatability.",
                "example": "1"
              },
              "time_limit": {
                "type": "integer",
                "description": "The time limit in milliseconds - if not completed within this time, generation will stop. The text generated so far will be returned along with the TIME_LIMIT stop reason.",
                "example": "600000"
              },
              "temperature": {
                "type": "number",
                "description": "The value used to module the next token probabilities. The range is 0.00 to 2.00, a value set to 0.00 would make it deterministic.",
                "example": "0.7"
              },
              "top_k": {
                "type": "integer",
                "description": "The number of highest probability vocabulary tokens to keep for top-k-filtering. Only applies for sampling mode.  The range is 1 to 100.",
                "example": "50"
              },
              "top_p": {
                "type": "number",
                "description": "Similar to top_k except the candidates to generate the next token are the most likely tokens with probabilities that add up to at least top_p. The range is 0.0  to 1.0 . A value of 1.0 is equivalent to disabled.",
                "example": "0.5"
              },
              "max_new_tokens": {
                "type": "number",
                "description": "The maximum number of new tokens to be generated.",
                "example": "150"
              },
              "min_new_tokens": {
                "type": "number",
                "description": "The minimum number of new tokens to be generated.",
                "example": "50"
              },
              "repetition_penalty": {
                "type": "number",
                "description": "The value which represents the penalty for penalizing tokens that have already been generated or belong to the context.",
                "example": "1.10"
              },
              "stop_sequences": {
                "type": "array",
                "items": {
                  "type": "string"
                },
                "description": "Stop sequences are one or more strings which will cause the text generation to stop if/when they are produced as part of the output. Stop sequences encountered prior to the minimum number of tokens being generated will be ignored.",
                "example": [
                  "\n\n"
                ]
              },
              "include_stop_sequence": {
                "type": "boolean",
                "description": "The value to control presence of matched stop sequences from the end of the output text. The default is true, meaning that the output will end with the stop sequence text when matched.",
                "example": "true"
              }
            }
          }
        }
      },
      "TextGenResponse": {
        "type": "object",
        "properties": {
          "model_id": {
            "description": "The ID of the model to be used for this request",
            "type": "string"
          },
          "created_at": {
            "description": "The date and time of the response",
            "type": "string"
          },
          "results": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "generated_text": {
                  "description": "The generated text",
                  "type": "string"
                },
                "generated_token_count": {
                  "description": "The number of tokens in the output",
                  "type": "integer"
                },
                "input_token_count": {
                  "description": "The number of tokens in the input",
                  "type": "integer"
                },
                "stop_reason": {
                  "description": "The reason for stopping the generation.  Can be NOT_FINISHED - Possibly more tokens to be streamed, MAX_TOKENS - Maximum requested tokens reached, EOS_TOKEN - End of sequence token encountered, CANCELLED - Request canceled by the client, TIME_LIMIT - Time limit reached, STOP_SEQUENCE - Stop sequence encountered, TOKEN_LIMIT - Token limit reached, ERROR - Error encountered",
                  "type": "string"
                }
              }
            }
          }
        }
      }
    },
    "securitySchemes": {
      "oauth2": {
        "type": "oauth2",
        "flows": {
          "x-apikey": {
            "tokenUrl": "https://iam.cloud.ibm.com/identity/token",
            "grantType": "urn:ibm:params:oauth:grant-type:apikey",
            "secretKeys": [
              "apikey"
            ],
            "paramKeys": [],
            "scopes": {}
          }
        }
      }
    }
  }
}